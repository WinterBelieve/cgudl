{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "In this step, we set up the basic execution environment. We define the GPU device if available, mount the project directory, download necessary datasets, and configure matplotlib settings. The autoreload extension is enabled to automatically reload modules if changes occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 1: Setup Cell\n",
    "# =============================================================================\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Set GPU device \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# Mount directory to ensure cgudl modules can be imported\n",
    "FOLDERNAME = 'cgudl/cgudl-assignment3/'\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "sys.path.append('/home/jovyan/{}'.format(FOLDERNAME))\n",
    "\n",
    "# Change directory and download datasets if necessary\n",
    "%cd /home/jovyan/$FOLDERNAME/cgudl/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd /home/jovyan/$FOLDERNAME\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Enable autoreload for convenience \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\"Return relative error between x and y.\"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Load COCO Dataset\n",
    "In this step, we load the COCO dataset using load_coco_data(pca_features=True) provided by cgudl. This function returns pre-extracted and PCA-compressed image features and captions (features typically have shape [N, 512] and captions [N, T]), along with a vocabulary dictionary for further decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Step 2: Load COCO Dataset\n",
    "# =============================================================================\n",
    "from cgudl.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from cgudl.image_utils import image_from_url\n",
    "\n",
    "# Load pre-extracted and PCA-compressed image features and captions\n",
    "data = load_coco_data(pca_features=True)\n",
    "print(\"Train features shape:\", data['train_features'].shape)\n",
    "print(\"Train captions shape:\", data['train_captions'].shape)\n",
    "print(\"Vocab size:\", len(data['word_to_idx']))\n",
    "\n",
    "word_to_idx = data['word_to_idx']\n",
    "idx_to_word = data['idx_to_word']\n",
    "vocab_size = len(word_to_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Data Inspection and Visualization\n",
    "In this step, we sample a small batch of data using sample_coco_minibatch and then use image_from_url and decode_captions to display the images along with their corresponding captions. This helps students visually verify that the data has been loaded correctly and understand its format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 3: Data Inspection and Visualization\n",
    "# =============================================================================\n",
    "# ex: batch_size = 32\n",
    "batch_size = 32\n",
    "# Set batch size to None to sample a single image\n",
    "captions_sample, features_sample, urls_sample = sample_coco_minibatch(data, batch_size=batch_size)\n",
    "for i, (caption, url) in enumerate(zip(captions_sample, urls_sample)):\n",
    "    try:\n",
    "        img = image_from_url(url)\n",
    "        if img is None:\n",
    "            print(f\"Image {i}: URL not found, skip.\")\n",
    "            continue\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        caption_str = decode_captions(caption, data['idx_to_word'])\n",
    "        plt.title(caption_str)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying image {i} from URL: {url}\\n\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Model Definition (Pure RNN Version) - TODO Version\n",
    "In this step, we define the model: the encoder and the decoder.\n",
    "\n",
    "The SimpleEncoder maps the pre-extracted features (e.g., 512-dim) via a linear layer and BatchNorm to the embedding space.\n",
    "\n",
    "The DecoderRNN uses a pure RNN (nn.RNN) to generate captions. In training, we split the captions into input (captions_in = captions[:, :-1]) and target (captions_out = captions[:, 1:]). The sample() method uses greedy search to generate captions.\n",
    "The core parts are marked with TODO, which students must fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Step 4: Model Definition (Pure RNN Version) - TODO Version\n",
    "# =============================================================================\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----- (4-1) Custom Dataset (Keep unchanged) -----\n",
    "class CocoFeatureCaptionDataset(Dataset):\n",
    "    def __init__(self, features, captions):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.captions = torch.tensor(captions, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.captions[idx]\n",
    "\n",
    "train_dataset = CocoFeatureCaptionDataset(data['train_features'], data['train_captions'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Settings\n",
    "The \"Parameter Settings\" section is used to define and organize the various hyperparameters required by the model. This part allows you to adjust key parameters that control the model architecture, training process, and data handling, such as the input feature dimension, embedding size, RNN hidden state size, number of layers, the maximum sequence length for caption generation, learning rate, and batch size. By centralizing these settings, users can conveniently modify the configuration for model tuning and experimental comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- Parameter Settings (TODO version) -----\n",
    "# TODO: Set your hyperparameters here.\n",
    "# input_dim = data['train_features'].shape[1]  # e.g., 512\n",
    "# embed_size \n",
    "# hidden_size \n",
    "# num_layers \n",
    "# max_seq_length = data['train_captions'].shape[1]  # e.g., 17\n",
    "# lr \n",
    "# batch_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- (4-2) SimpleEncoder -----\n",
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_size):\n",
    "        super(SimpleEncoder, self).__init__()\n",
    "        # TODO: Define a Linear layer to map input_dim to embed_size\n",
    "\n",
    "        # TODO: Define a BatchNorm1d layer with num_features=embed_size\n",
    "\n",
    "        pass\n",
    "\n",
    "    def forward(self, features):\n",
    "        out = self.linear(features)\n",
    "        out = self.bn(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- (4-3) DecoderRNN -----\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        \"\"\"\n",
    "        captions include <START> and <END> tokens, with shape (batch, T).\n",
    "        We define:\n",
    "          captions_in = captions[:, :-1]  (input)\n",
    "          captions_out = captions[:, 1:]   (target)\n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def forward(self, features, captions_in):\n",
    "        \"\"\"\n",
    "        features: (batch, embed_size)\n",
    "        captions_in: (batch, T-1) (input portion, without the last token)\n",
    "        Returns:\n",
    "            outputs: (batch, T-1, vocab_size)\n",
    "        \"\"\"\n",
    "        # TODO: Use the Embedding layer to convert captions_in to embeddings; shape should be (batch, T-1, embed_size)\n",
    "        pass\n",
    "        # TODO: Initialize the RNN hidden state using features (repeat for num_layers)\n",
    "        pass\n",
    "        # TODO: Pass embeddings and hidden state through the RNN layer, then apply the Linear layer to produce vocab scores\n",
    "        pass\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"\n",
    "        Use greedy search to generate caption without teacher forcing.\n",
    "        \"\"\"\n",
    "        batch_size = features.size(0)\n",
    "        sampled_ids = []\n",
    "        # Get Original token ID (According to the dictionary, assume it is '<START>' or '<start>'; if not set, default to 1)\n",
    "        start_token = word_to_idx.get('<START>', word_to_idx.get('<start>', 1))\n",
    "        inputs = self.embed(torch.tensor([start_token] * batch_size, device=device))\n",
    "        inputs = inputs.unsqueeze(1)  # (batch, 1, embed_size)\n",
    "        states = features.unsqueeze(0).repeat(self.rnn.num_layers, 1, 1)\n",
    "        for i in range(self.max_seq_length):\n",
    "            hiddens, states = self.rnn(inputs, states)\n",
    "            outputs = self.linear(hiddens.squeeze(1))\n",
    "            _, predicted = outputs.max(1)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- (4-4) ImageCaptioningModel -----\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        # TODO: Instantiate SimpleEncoder and DecoderRNN, and assign them to self.encoder and self.decoder\n",
    "        pass\n",
    "        \n",
    "    def forward(self, features, captions, lengths=None):\n",
    "        \"\"\"\n",
    "        captions: (batch, T) containing <START> and <END> tokens.\n",
    "        \"\"\"\n",
    "        # TODO: Split captions into captions_in and pass features and captions_in to the decoder; return the outputs.\n",
    "        pass\n",
    "    \n",
    "    def sample(self, features):\n",
    "        return self.decoder.sample(self.encoder(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageCaptioningModel(input_dim, embed_size, hidden_size, vocab_size, num_layers, max_seq_length)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Training Loop\n",
    "In this step, we run the training loop using tqdm to track progress. For each batch, we compute the loss and update the model parameters. After every epoch, the average loss is recorded and sample captions are generated and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 5: Training Loop (using torch autograd and tqdm)\n",
    "# =============================================================================\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set loss and optimizer\n",
    "# According to cgudl data, the padding token is '<NULL>'\n",
    "pad_token = word_to_idx.get('<NULL>', 0)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "        for features, captions in train_loader:\n",
    "            features = features.to(device)\n",
    "            captions = captions.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Split captions into input and target: captions_in = captions[:, :-1], targets = captions[:, 1:]\n",
    "            outputs = model(features, captions, None)  # outputs: (batch, T-1, vocab_size)\n",
    "            targets = captions[:, 1:]\n",
    "            loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "            pbar.update(1)\n",
    "    epoch_loss /= len(train_loader)\n",
    "    loss_history.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch} loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # Sample and display generated captions after each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_features, _ = next(iter(train_loader))\n",
    "        sample_features = sample_features.to(device)\n",
    "        sampled_ids = model.sample(sample_features)  # (batch, max_seq_length)\n",
    "        sampled_ids = sampled_ids.cpu().numpy()\n",
    "        captions_generated = decode_captions(sampled_ids, idx_to_word)\n",
    "        print(\"Sample captions:\")\n",
    "        for cap in captions_generated[:4]:\n",
    "            print(cap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Loss Plot and Sample Image Display\n",
    "In the final step, we plot the training loss curve using matplotlib and display sample images from the validation set along with the generated and ground truth captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Step 6: Plot Loss Curve and Display Sample Images with Captions\n",
    "# =============================================================================\n",
    "plt.figure()\n",
    "plt.plot(loss_history, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss History')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    gt_captions, features, urls = sample_coco_minibatch(data, split='val', batch_size=batch_size)\n",
    "    features = torch.tensor(features, dtype=torch.float32).to(device)\n",
    "    sampled_ids = model.sample(features)  # (batch, max_seq_length)\n",
    "    sampled_ids = sampled_ids.cpu().numpy()\n",
    "    generated_captions = decode_captions(sampled_ids, idx_to_word)\n",
    "    gt_text = decode_captions(gt_captions, data['idx_to_word'])\n",
    "    \n",
    "    for gt_caption, gen_caption, url in zip(gt_text, generated_captions, urls):\n",
    "        try:\n",
    "            img = image_from_url(url)\n",
    "            if img is None:\n",
    "                print(\"Image not found:\", url)\n",
    "                continue\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(\"GT: \" + gt_caption + \"\\nGen: \" + gen_caption)\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error displaying image {url}:\\n\", e)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
